{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bc0872a",
   "metadata": {},
   "source": [
    "# Illustration of functions approximated by a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4da7c77",
   "metadata": {},
   "source": [
    "In this notebook we try to build some intuition for the nature of neural networks as “universal function approximators”. We will explore how the input get warped by a multilayer perceptron that has been initialized with random weights. We restrict ourselves to input and output that can be easily plotted.\n",
    "\n",
    "## 1D Example\n",
    "\n",
    "The simplest case is the $\\mathbb{R} \\rightarrow \\mathbb{R}$ or in more familiar form $y = f(x)$. We will see how a fully connected neural network warps this input for $y = mx + c$. \n",
    "\n",
    "Check out how adding more layers or changing the nodes per layer heaps on more twisting and distorting.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babb39a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup some imports and basic plotting parameters\n",
    "\n",
    "import numpy\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams.update(\n",
    "    { 'font.size': 14,\n",
    "      'font.family': \"sans-serif\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1baeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multilayer_perceptron(n_network, n_class, n_hidden_layers, n_hidden_dimension, activation='tanh'):\n",
    "    \"\"\"Create a multi-layer perceptron\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_network: int\n",
    "        \n",
    "    n_class: int\n",
    "    \n",
    "    n_hidden_layers: int\n",
    "        The number of hidden layers in the perceptron\n",
    "        \n",
    "    n_hidden_dimension: int\n",
    "        The number of nodes in each of the hidden layers\n",
    "        \n",
    "    activation: str\n",
    "        Activation function. One of `tanh` or `relu`\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    model = torch.nn\n",
    "        A pytorch neural network\n",
    "    \"\"\"\n",
    "    if activation == 'tanh':\n",
    "        activationfn = torch.nn.Tanh()\n",
    "    elif activation == 'relu':\n",
    "        activationfn = torch.nn.ReLU()\n",
    "    else:\n",
    "        raise NotImplementedError('Activation function not implemented')\n",
    "        \n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(n_network, n_hidden_dimension),\n",
    "        activationfn)\n",
    "    \n",
    "    for i in range(n_hidden_layers - 1):\n",
    "        model = torch.nn.Sequential(\n",
    "            model,\n",
    "            torch.nn.Linear(n_hidden_dimension, n_hidden_dimension),\n",
    "            activationfn)\n",
    "\n",
    "    model = torch.nn.Sequential(\n",
    "        model, torch.nn.Linear(n_hidden_dimension, n_class))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    \"\"\"Initialize the weights of a neural network\n",
    "    \n",
    "    This function initializes the parameters of the neural network from \n",
    "    a Normal distribution with 0 mean and 0.5 standard deviation. Both the\n",
    "    weights and biases are initialized using this distribution.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    m: torch.nn\n",
    "        The neural network whose parameters are being initialized\n",
    "    \"\"\"\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.normal_(m.weight, 0, 0.5)\n",
    "        m.bias.data.normal_(0, 0.5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b035f29",
   "metadata": {},
   "source": [
    "### Define the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf5d93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 400\n",
    "d = 1.0\n",
    "x = torch.linspace(-d, d, N)\n",
    "\n",
    "# Setup a line - x = y example\n",
    "y = x.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0acd70",
   "metadata": {},
   "source": [
    "### Effect of activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3693a43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the neural network parameters\n",
    "num_layers = 6\n",
    "n_hidden_dimension = 32\n",
    "activation_function = 'tanh' # 'relu'\n",
    "\n",
    "# Setup the plotting\n",
    "fig = plt.figure(figsize=(16.0,7.0))\n",
    "gs = gridspec.GridSpec(1,2)\n",
    "ax1 = fig.add_subplot(gs[0,0])\n",
    "ax2 = fig.add_subplot(gs[0,1])\n",
    "ax1.plot(x,y, color='k', ls='--')\n",
    "ax2.plot(x,y, color='k', ls='--')\n",
    "fig.suptitle('Random neural networks as function approximators')\n",
    "\n",
    "# Loop over different activation functions and random initializations\n",
    "# to generate outputs\n",
    "nruns = 5\n",
    "colors = plt.cm.viridis(numpy.linspace(0,1,nruns))\n",
    "for ax, afn in zip([ax1, ax2],['tanh','relu']):\n",
    "    ax.set_title(f'Using activation function : {afn}')\n",
    "    net = get_multilayer_perceptron(1,1,num_layers,n_hidden_dimension,afn)\n",
    "    for run in range(nruns):\n",
    "        net.apply(init_weights)\n",
    "        y_new = []    \n",
    "        for i in x:\n",
    "            i = torch.Tensor([i])\n",
    "            y_new.append(torch.squeeze(net(i)).detach().numpy())\n",
    "    \n",
    "        y_new = numpy.array(y_new).flatten()\n",
    "        ax.plot(x,y_new, color=colors[run], lw=2, alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923c219f",
   "metadata": {},
   "source": [
    "### Effect of the number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cafde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16.0,7.0))\n",
    "gs = gridspec.GridSpec(1,2)\n",
    "ax1 = fig.add_subplot(gs[0,0])\n",
    "ax2 = fig.add_subplot(gs[0,1])\n",
    "\n",
    "ax1.plot(x,y, color='k', ls='--')\n",
    "ax2.plot(x,y, color='k', ls='--')\n",
    "\n",
    "nruns = 10\n",
    "colors = plt.cm.Spectral(numpy.linspace(0,1,nruns))\n",
    "for ax, num_layers in zip([ax1, ax2],[3,7]):\n",
    "    ax.set_title(f'Number of layers : {num_layers}')\n",
    "    net = get_multilayer_perceptron(1,1,num_layers,n_hidden_dimension,'tanh')\n",
    "    for run in range(nruns):\n",
    "        net.apply(init_weights)\n",
    "        y_new = []    \n",
    "        for i in x:\n",
    "            i = torch.Tensor([i])\n",
    "            y_new.append(torch.squeeze(net(i)).detach().numpy())\n",
    "    \n",
    "        y_new = numpy.array(y_new).flatten()\n",
    "        ax.plot(x,y_new, color=colors[run],alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3c11aa",
   "metadata": {},
   "source": [
    "## 2D Example\n",
    "\n",
    "The second example is mapping a 2D input to a 2D output, i.e., $\\mathbb{R}^{2} \\rightarrow \\mathbb{R}^{2}$. The visualization below shows the warping of the area $[-1, 1]^2$ after being fed through a multilayer perceptron initialized using the same method as the 1D Example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe067795",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "d = 1.0\n",
    "x = torch.linspace(-d, d, N)\n",
    "y = torch.linspace(-d, d, N)\n",
    "\n",
    "xx = torch.matmul(torch.ones([N, 1]), x.reshape([1, N]))\n",
    "yy = torch.matmul(y.reshape([N, 1]), torch.ones([1, N]));\n",
    "xx = torch.reshape(xx, [-1]);\n",
    "yy = torch.reshape(yy, [-1]);\n",
    "\n",
    "net3 = get_multilayer_perceptron(2,1,3,n_hidden_dimension,'tanh')\n",
    "net7 = get_multilayer_perceptron(2,1,7,n_hidden_dimension,'tanh')\n",
    "\n",
    "net3.apply(init_weights)\n",
    "net7.apply(init_weights)\n",
    "\n",
    "zi3 = []\n",
    "zi7 = []\n",
    "for xi, yi in zip(xx, yy):\n",
    "    i = torch.Tensor([xi, yi])\n",
    "    zi3.append(torch.squeeze(net3(i)).detach().numpy())\n",
    "    zi7.append(torch.squeeze(net7(i)).detach().numpy())\n",
    "\n",
    "print(xx.shape, yy.shape)\n",
    "zi3 = numpy.array(zi3).flatten()\n",
    "zi7 = numpy.array(zi7).flatten()\n",
    "\n",
    "fig = plt.figure(figsize=(16.0,6.0))\n",
    "gs = gridspec.GridSpec(1,2)\n",
    "ax1 = fig.add_subplot(gs[0,0], projection='3d')\n",
    "ax2 = fig.add_subplot(gs[0,1], projection='3d')\n",
    "\n",
    "ax1.plot_trisurf(xx, yy, zi3, cmap='viridis', edgecolor='none')\n",
    "ax2.plot_trisurf(xx, yy, zi7, cmap='viridis', edgecolor='none')\n",
    "ax1.set_title(f'Number of layers: 3')\n",
    "ax2.set_title(f'Number of layers: 7')\n",
    "\n",
    "ax1.set_axis_off()\n",
    "ax2.set_axis_off()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f63d32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16.0,6.0))\n",
    "gs = gridspec.GridSpec(1,2)\n",
    "ax1 = fig.add_subplot(gs[0,0])\n",
    "ax2 = fig.add_subplot(gs[0,1])\n",
    "ax1.tripcolor(xx, yy, zi3, cmap = plt.get_cmap('YlGnBu_r'))\n",
    "ax2.tripcolor(xx, yy, zi7, cmap = plt.get_cmap('YlGnBu_r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559d5e7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
