{"cells":[{"cell_type":"markdown","metadata":{"id":"0DGbUGilSQXc"},"source":["# Chapter 2 - Gaussian mixture model\n","\n","\n","## Data generation\n","\n","1. 10 means $m_k$ were drawn from a bivariate Gaussian distribution $\\mathcal{N}((1,0),I)$ and labeled *BLUE*. \n","2. 10 more were drawn from $\\mathcal{N}((0,1),I)$ and labeled *ORANGE*. \n","3. For each class 100 observations were generated as follows: \n","   * $m_k$ was picked at random with probability 1/10;\n","   * observation was drawn from $\\mathcal{N}(m_k,I/5)$, thus leading to a mixture of Gaussian clusters for each class."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P7J3JkEPSQXp"},"outputs":[],"source":["import numpy\n","from matplotlib import pyplot as plt\n","\n","%matplotlib inline\n","\n","# define commonly used colors\n","GRAY1 = '#231F20'\n","GRAY4 = '#646369'\n","PURPLE = '#A020F0'\n","BLUE =  '#57B5E8'\n","ORANGE = '#E69E00'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mBczek60SQXr","outputId":"2ad8ef9d-e6ff-4d0f-ea6a-70dc3cfd8b70"},"outputs":[],"source":["# Generate the means\n","N_means = 10\n","N_data = 100\n","means_blue = numpy.random.multivariate_normal((1,0),[[1,0],[0,1]], N_means)\n","means_orange = numpy.random.multivariate_normal((0,1),[[1,0],[0,1]], N_means)\n","means_all = numpy.vstack((means_blue, means_orange))\n","\n","# Generate the data\n","# Need to make this better rather than \n","bdata = []\n","odata = []\n","for i in range(N_data):\n","    mb = means_blue[numpy.random.choice(10)]\n","    mo = means_orange[numpy.random.choice(10)]\n","    b1 = numpy.random.multivariate_normal(mb,[[1/5,0],[0,1/5]], 1)\n","    o1 = numpy.random.multivariate_normal(mo,[[1/5,0],[0,1/5]], 1)\n","    bdata.append(b1[0])\n","    odata.append(o1[0])\n","    \n","bdata = numpy.array(bdata)\n","odata = numpy.array(odata)\n","\n","X_train = numpy.vstack((bdata, odata))\n","y_train = numpy.concatenate((numpy.zeros(N_data, dtype=numpy.int), numpy.zeros(N_data, dtype=numpy.int)+1))\n","\n","# Plot the data\n","# prepares a plot with a title and circles representing training data\n","def plot_train_data(title):\n","    fig, ax = plt.subplots(figsize=(2.8, 2.8), dpi=110)\n","    ax.set_aspect(1.3)\n","    ax.scatter(X_train[:, 0], X_train[:, 1], s=18, facecolors='none', edgecolors=numpy.array([BLUE, ORANGE])[y_train].flatten())\n","    ax.tick_params(bottom=False, left=False, labelleft=False, labelbottom=False)\n","    ax.set_xlim(-2.6, 4.2)\n","    ax.set_ylim(-2.0, 2.9)\n","    fig.subplots_adjust(left=0, right=1, top=1, bottom=0)\n","    ax.text(-2.6, 3.2, title, color=GRAY4, fontsize=9)\n","    for spine in ax.spines.values():\n","        spine.set_color(GRAY1)\n","    return fig, ax\n","\n","\n","# test it\n","_, _ = plot_train_data('Training data')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yUW3DwVUSQXu","outputId":"f29022c2-5586-4f6f-9dc5-320a02c6bdf1"},"outputs":[],"source":["from sklearn.mixture import GaussianMixture\n","from sklearn.metrics import accuracy_score\n","from sklearn.mixture.gaussian_mixture import _compute_precision_cholesky\n","import pandas\n","\n","# even though we already know means and covariances, we need to\n","# do \"fake fit\", otherwise GaussianMixture model will not work\n","gaussian_mixture_model = GaussianMixture(\n","    n_components=20,\n","    covariance_type='spherical',\n","    means_init=means_all,\n","    random_state=1\n",").fit(means_all)\n","# set known covariances\n","gaussian_mixture_model.covariances_ = [1/5]*20"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Csj4FQuSSQXu"},"outputs":[],"source":["# This is hacky! \n","# GaussianMixture uses precisions_cholesky_ for predict_proba method. \n","# We need to recalculate precisions_cholesky_ since we changed covariances_.\n","gaussian_mixture_model.precisions_cholesky_ = _compute_precision_cholesky(\n","    gaussian_mixture_model.covariances_,  \n","    gaussian_mixture_model.covariance_type\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5VYDU37sSQXv"},"outputs":[],"source":["# Sample 10000 points for testing\n","X_test, y_test = gaussian_mixture_model.sample(10000)\n","\n","# y_test contains sampled component indices\n","# index < 10 means that the class is BLUE (0)\n","y_test = 1*(y_test >= 10)"]},{"cell_type":"markdown","metadata":{"id":"7d9g5Iy-SQXw"},"source":["## Setup some plotting functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A6lXtGDESQXw"},"outputs":[],"source":["# given a model prediction function computes X points on n x n grid and the\n","# corresponding predicted classes\n","def fill_prediction_grid(n1, n2, predict):\n","    x1, x2 = numpy.linspace(-2.6, 4.2, n1), numpy.linspace(-2.0, 2.9, n2)\n","    X = numpy.transpose([numpy.tile(x1, n2), numpy.repeat(x2, n1)])\n","    y = predict(X)\n","    return X, y\n","\n","\n","# given a model prediction function computes X0 and X1 n x n meshgrids\n","# and the corresponing predicted classes meshgrid\n","def fill_prediction_meshgrid(predict):\n","    n1, n2 = 1000, 1000\n","    X, y = fill_prediction_grid(n1, n2, predict)\n","    return X[:, 0].reshape(n1, n2), X[:, 1].reshape(n1, n2), y.reshape(n1, n2)\n","\n","\n","# given a model prediction function plots train data, model decision\n","# bounary and background dots\n","def plot_model(predict, title):\n","    fig, ax = plot_train_data(title)\n","    # plot background dots\n","    X, y = fill_prediction_grid(69, 99, predict)\n","    ax.scatter(X[:, 0], X[:, 1], marker='.', lw=0, s=2,\n","               c=numpy.array([BLUE, ORANGE])[y])\n","    # plot the decision boundary\n","    X0, X1, Y = fill_prediction_meshgrid(predict)\n","    ax.contour(X0, X1, Y, [0.5], colors=GRAY1, linewidths=[0.7])\n","    return fig, ax\n","\n","# given a model prediction function plots performance statistics\n","def plot_model_stat(predict, title, bayes=None):\n","    fig, ax = plot_model(predict, title)\n","    test_error_rate = 1 - accuracy_score(y_test, predict(X_test))\n","    train_error_rate = 1 - accuracy_score(y_train, predict(X_train))\n","    parms = {'color': GRAY1, 'fontsize': 7,\n","             'bbox': {'facecolor': 'white', 'pad': 3, 'edgecolor': 'none'}}\n","    ax.text(-2.42, -1.35, f'Training Error: {train_error_rate:.3f}', **parms)\n","    ax.text(-2.42, -1.62, f'Test Error:       {test_error_rate:.3f}', **parms)\n","    return fig, ax"]},{"cell_type":"markdown","metadata":{"id":"e5qf8i1PSQXx"},"source":["<h2>Linear Regression</h2>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3HHrIiF_SQXy"},"outputs":[],"source":["from sklearn.linear_model import LinearRegression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WlymfIa3SQXz","outputId":"64b80e17-74c8-4485-cecb-96e86cbc205d"},"outputs":[],"source":["# PAGE 12. Let’s look at an example of the linear model in a classiﬁcation\n","#          context.\n","linear_regression_model = LinearRegression().fit(X_train, y_train)\n","\n","\n","# PAGE 12. The fitted values Y-hat are converted to a fitted class variable\n","#          G-hat according to the rule G-hat = (ORANGE if Y-hat > 0.5, BLUE if\n","#          Y-hat ≤ 0.5.\n","def linear_predict(X):\n","    return 1*(linear_regression_model.predict(X) > 0.5)\n","\n","# PAGE 13. The line is the decision boundary deﬁned by x.T @ b = 0.5. The\n","#          orange shaded region denotes that part of input space classiﬁed as\n","#          ORANGE, while the blue region is classiﬁed as BLUE.\n","_, _ = plot_model_stat(linear_predict, 'Linear Regression of 0/1 Response')"]},{"cell_type":"markdown","metadata":{"id":"us2S-J3nSQXz"},"source":["<h2>Nearest-Neighbor Methods</h2>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VzR3ccEDSQX0"},"outputs":[],"source":["from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.model_selection import GridSearchCV"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RhJFbU11SQX0","outputId":"4b34fc1f-1a5d-4b75-f9fa-8f7ef5349a70"},"outputs":[],"source":["# Run GridSearchCV to find the best n_neighbors parameter using the 10-folds\n","# CV. It finds 12, but the book uses 15-Nearest Neighbor Classifier because\n","# the authors selected the most parsimonious model within one standard error\n","# from the best model (one standard error rule). We will apply this rule in\n","# other examples, not here.\n","k_neighbors_grid_search = GridSearchCV(KNeighborsClassifier(),\n","                                       {'n_neighbors': list(range(1, 50))}, cv=10\n","                                      ).fit(X_train, y_train)\n","k_neighbors_grid_search.best_params_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q1RxFgRGSQX1","outputId":"db762453-1b40-43c7-8ba8-b49c8a199cc5"},"outputs":[],"source":["# PAGE 14. Use 15-nearest-neighbor averaging of the binary coded response as\n","#          the method of fitting. Thus Y-hat is the proportion of ORANGE’s in\n","#          the neighborhood, and so assigning class ORANGE to G-hat if\n","#          Y-hat>0.5 amounts to a majority vote in the neighborhood.\n","neighbors5_classifier = KNeighborsClassifier(n_neighbors=5).fit(X_train, y_train)\n","_, _ = plot_model_stat(neighbors5_classifier.predict, '5-Nearest Neighbor Classifier')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AfNZLSvISQX2","outputId":"dd5a67d7-1590-49ae-b8b9-be760c67539d"},"outputs":[],"source":["# PAGE 16. The classes are coded as a binary variable (BLUE = 0,ORANGE = 1),\n","#          and then predicted by 1-nearest-neighbor classiﬁcation.\n","neighbors1_classifier = KNeighborsClassifier(n_neighbors=1).fit(X_train, y_train)\n","_, _ = plot_model_stat(neighbors1_classifier.predict, '1−Nearest Neighbor Classifier')"]},{"cell_type":"markdown","metadata":{"id":"gXgOsCPTSQX2"},"source":["## Optimal Bayes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GVpt1cC8SQX3"},"outputs":[],"source":["def optimal_bayes_predict(X):\n","    components_proba = gaussian_mixture_model.predict_proba(X)\n","    # first 10 components are BLUE(0), and others are BROWN(1)\n","    blue_proba = numpy.sum(components_proba[:, :10], axis=1)\n","    brown_proba = numpy.sum(components_proba[:, 10:], axis=1)\n","    y_hat = 1*(blue_proba < brown_proba)\n","    return y_hat"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bzhcz7IYSQX3","outputId":"8e730c31-9a1e-4daa-81a7-56dcca9190e4"},"outputs":[],"source":["bayes_error_rate = 1 - accuracy_score(y_test, optimal_bayes_predict(X_test))\n","print(f'The optimal Bayes error rate = {bayes_error_rate}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YJrLaRPASQX4","outputId":"9b9611c1-cb02-4a21-d3d7-5f25d1f7a472"},"outputs":[],"source":["# plot the optimal Bayes decision boundary\n","_, _ = plot_model_stat(optimal_bayes_predict, 'Bayes Optimal Classifier')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0QfuWfMbSQX4"},"outputs":[],"source":["# lets save Bayes meshgrids for optimal decision boundary plotting\n","X0_bayes, X1_bayes, Y_bayes = fill_prediction_meshgrid(optimal_bayes_predict)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VcSVAiRESQX4"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Chapter2_GaussianMixtureExample.ipynb","provenance":[]},"kernelspec":{"display_name":"Python [conda env:py3_ds]","language":"python","name":"conda-env-py3_ds-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"nbformat":4,"nbformat_minor":0}
